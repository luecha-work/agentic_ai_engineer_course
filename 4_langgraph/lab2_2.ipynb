{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f58da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import gradio as gr\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from typing import Annotated, TypedDict\n",
    "import getpass4\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c508bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass4.getpass(\"Enter your Google AI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9fd1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "serper = GoogleSerperAPIWrapper()\n",
    "# serper.run(\"What is the capital of France?\")\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1730d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "tool_search = Tool(\n",
    "    name=\"search\",\n",
    "    description=\"Useful for when you need more information from an online search\",\n",
    "    func=serper.run,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3391d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tool_search.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedcec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_math(expression: str) -> str:\n",
    "    \"\"\"Calculate mathematical expressions safely\"\"\"\n",
    "    try:\n",
    "        # Remove any potentially dangerous characters and evaluate safely\n",
    "        allowed_chars: set[str] = set(\"0123456789+-*/(). \")\n",
    "        if all(c in allowed_chars for c in expression):\n",
    "            result = eval(expression)\n",
    "            return f\"ผลลัพธ์: {expression} = {result}\"\n",
    "        else:\n",
    "            return \"ข้อผิดพลาด: นิพจน์ไม่ถูกต้อง ใช้ได้เฉพาะตัวเลขและเครื่องหมาย +, -, *, /, (, ) เท่านั้น\"\n",
    "    except Exception as e:\n",
    "        return f\"ข้อผิดพลาดในการคำนวณ: {str(e)}\"\n",
    "\n",
    "\n",
    "calculate_tool = Tool(\n",
    "    name=\"calculator\",\n",
    "    func=calculate_math,\n",
    "    description=\"Use this to calculate mathematical expressions. Input should be a valid math expression like '2 + 3 * 4' or '(10 + 5) / 3'\",\n",
    ")\n",
    "\n",
    "calculate_tool.invoke(\"1+2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15002d5",
   "metadata": {},
   "source": [
    "#### Create Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85105b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [calculate_tool, tool_search]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68dcb66",
   "metadata": {},
   "source": [
    "#### Step 1: Define the State object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de961134",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388308aa",
   "metadata": {},
   "source": [
    "#### Step 2: Start the Graph Builder with this State class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb18812",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31146a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"gpt-oss\")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf48b3dc",
   "metadata": {},
   "source": [
    "#### Step 3: Create a Node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1b4456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State) -> State:\n",
    "    \"\"\"This function is called when the user sends a message to the chatbot.\"\"\"\n",
    "\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=tools))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216230f8",
   "metadata": {},
   "source": [
    "### Step 4: Create Edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580b1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_conditional_edges(\"chatbot\", tools_condition, {\"tools\": \"tools\"})\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44e6644",
   "metadata": {},
   "source": [
    "### Step 5: Compli the Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca1486",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba22478",
   "metadata": {},
   "source": [
    "#### This's is? And, let's do this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4f721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"123\"}}\n",
    "\n",
    "def chat(user_input: str, history) -> str:\n",
    "    \"\"\"This function is called when the user sends a message to the chatbot.\"\"\"\n",
    "    result = graph.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config=config\n",
    "    )\n",
    "    print(result)\n",
    "    \n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b8a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.get_state(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119a096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(graph.get_state_history(config))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc263292",
   "metadata": {},
   "source": [
    "## OK it's time to add Memory!\n",
    "\n",
    "### BUT WAIT!\n",
    "\n",
    "We have this whole Graph maintaining the state and appending to the state.  \n",
    "**Why isn't this handling memory?**\n",
    "\n",
    "---\n",
    "\n",
    "## This is a crucial point for understanding LangGraph\n",
    "\n",
    "> A super-step can be considered a single iteration over the graph nodes.  \n",
    "> Nodes that run in parallel are part of the same super-step,  \n",
    "> while nodes that run sequentially belong to separate super-steps.\n",
    "\n",
    "- One **\"Super-Step\"** of the graph represents one invocation of passing messages between agents.\n",
    "- In idiomatic LangGraph, you call `invoke()` to run your graph for each super-step; for each interaction.\n",
    "- The reducer handles state updates automatically **within one super-step**, but **not between them**.\n",
    "\n",
    "That is what **checkpointing** achieves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f42dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory: MemorySaver = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361132c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1\")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State) -> State:\n",
    "    \"\"\"This function is called when the user sends a message to the chatbot.\"\"\"\n",
    "\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=tools))\n",
    "\n",
    "graph_builder.add_conditional_edges(\"chatbot\", tools_condition, {\"tools\": \"tools\"})\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "# graph_builder.add_edge(\"chatbot\", END)\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf4d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"222\"}}\n",
    "\n",
    "def chat(user_input: str, history) -> str:\n",
    "    \"\"\"This function is called when the user sends a message to the chatbot.\"\"\"\n",
    "    result = graph.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config=config\n",
    "    )\n",
    "    print(result)\n",
    "    \n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf8cfc",
   "metadata": {},
   "source": [
    "### LangChain gives you tools to set the state back to a prior point in time, to branch off:\n",
    "\n",
    "```\n",
    "config = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": ...}}\n",
    "graph.invoke(None, config=config)\n",
    "```\n",
    "\n",
    "And this allows you to build stable systems that can be recovered and rerun from any prior checkpoint.\n",
    "\n",
    "And now let's store in SQL\n",
    "\n",
    "And this is the power of LangGraph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7dc458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "\n",
    "db_path = \"memory.db\"\n",
    "conn = sqlite3.connect(db_path, check_same_thread=False)\n",
    "sql_memory = SqliteSaver(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aafb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps 1 and 2\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Step 3\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    print(state)\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=tools))\n",
    "\n",
    "# Step 4\n",
    "# graph_builder.add_conditional_edges(\"chatbot\", tools_condition, {\"tools\": \"tools\"})\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\", tools_condition, {\"tools\": \"tools\", \"__end__\": END}\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# graph_builder.add_conditional_edges(\"chatbot\", tools_condition, {\"tools\": \"tools\"})\n",
    "# graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "# graph_builder.add_edge(START, \"chatbot\")\n",
    "\n",
    "# Step 5\n",
    "graph = graph_builder.compile(checkpointer=sql_memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dad70a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3333\"}}\n",
    "\n",
    "\n",
    "def chat(user_input: str, history) -> str:\n",
    "    \"\"\"This function is called when the user sends a message to the chatbot.\"\"\"\n",
    "    result = graph.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config=config\n",
    "    )\n",
    "    print(result)\n",
    "    \n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
